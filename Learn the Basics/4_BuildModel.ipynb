{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f7cee4",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "- 神经网络由对数据执行操作的layers/modules组成。`torch.nn` 命名空间提供了你构建自己的神经网络所需的所有基础组件(building blocks)。\n",
    "- PyTorch 中的每个module都是 `nn.Module` 的子类。\n",
    "- 神经网络本身就是一个module，它由其他modules（layerd）组成。这种嵌套结构使得轻松构建和管理复杂的架构成为可能。\n",
    "\n",
    "在接下来的章节中，我们将构建一个神经网络，对 FashionMNIST 数据集中的图像进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5ab8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d7d3f",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "我们希望能够在诸如 CUDA、MPS、MTIA 或 XPU 等加速器上训练我们的模型。如果当前的加速器可用，我们就会使用它。否则，我们将使用 CPU。\n",
    "- 具体来说，CUDA 是 NVIDIA 推出的一种并行计算平台和编程模型，可加速 GPU 计算；\n",
    "- MPS 是苹果公司的 Metal Performance Shaders，用于在苹果设备 GPU 上进行高性能计算；\n",
    "- MTIA 是英特尔推出的一种面向 AI 推理的加速器；\n",
    "- XPU 是一些厂商自定义的通用计算加速芯片的统称。\n",
    "\n",
    "这句话表明训练模型优先选择可用的这些加速器，若都不可用则退而求其次使用中央处理器（CPU）来训练模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0af4fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb2d6c",
   "metadata": {},
   "source": [
    "## Define the class\n",
    "1. 我们通过继承 `nn.Module` 类来定义神经网络，并在`__init__`方法中初始化神经网络的各层。每个 `nn.Module` 的子类都要在 `forward`方法中实现对输入数据的操作。\n",
    "2. 然后定义了一个名为 `NeuralNetwork` 的类，它继承自`nn.Module`。\n",
    "- 具体来说，在 **PyTorch** 框架中，`nn.Module` 是所有神经网络模块的基类。当创建自定义神经网络时，通常会创建一个类继承 `nn.Module`。在这个类的`__init__`函数里，会定义神经网络的各个层，比如全连接层、卷积层等。\n",
    "- 而 `forward` 函数则定义了数据在网络中的前向传播路径，也就是输入数据如何经过各个层的运算得到输出。\n",
    "    例如，可能在`__init__`里定义一个线性层```self.linear = nn.Linear(10, 5)```，表示输入维度为 10，输出维度为 5 的线性层，然后在 `forward` 里定义```x = self.linear(x)```来实现数据的前向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230772e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    # 定义神经网络类，继承自PyTorch的nn.Module基类\n",
    "    def __init__(self):\n",
    "        # 类初始化方法\n",
    "        super().__init__()  # 调用父类nn.Module的初始化方法, \n",
    "        self.flatten = nn.Flatten()  # 创建Flatten层，用于将输入图像展平为一维向量\n",
    "        \n",
    "        # 创建Sequential容器，按顺序包含多个神经网络层\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # 第一个全连接层，将28×28=784维输入映射到512维隐藏层\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),  # ReLU激活函数，引入非线性特性\n",
    "            \n",
    "            # 第二个全连接层，将512维输入映射到512维隐藏层\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),  # ReLU激活函数\n",
    "            \n",
    "            # 第三个全连接层，将512维输入映射到10维输出（对应10个类别）\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播过程\n",
    "        x = self.flatten(x)  # 将输入图像展平为一维向量\n",
    "        logits = self.linear_relu_stack(x)  # 依次通过线性层和激活函数\n",
    "        return logits  # 返回最终的未归一化预测值（logits）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e235561f",
   "metadata": {},
   "source": [
    "> “super ()” 是 Python 的一个内置函数，用于调用父类（超类）的方法。它常出现在类的继承体系中，当子类需要复用父类的方法，同时又要添加一些额外的功能时使用。\n",
    ">\n",
    "> “全连接层” 指在神经网络中，**该层的每个神经元都与上一层的所有神经元相连接**。这种连接方式使得全连接层能够整合上一层的所有信息，从而学习到数据中的复杂模式。例如在图像识别任务里，经过卷积层和池化层提取特征后，全连接层可以把这些特征组合起来用于图像分类。它在神经网络中常用于将提取到的特征映射到最终的输出空间，如用于分类任务时，将特征映射到不同类别对应的概率值\n",
    ">\n",
    ">ReLU 即修正线性单元（Rectified Linear Unit ）激活函数，是一种在人工神经网络中广泛使用的激活函数。它的数学表达式为：f (x) = max (0, x) ，意思是当输入值 x 大于 0 时，输出就是 x 本身；当输入值 x 小于等于 0 时，输出为 0 。\n",
    "ReLU 激活函数在神经网络中有诸多优点。比如，它能有效解决梯度消失问题，使得网络在训练过程中更容易收敛。在图像识别领域，使用 ReLU 激活函数的卷积神经网络可以更好地对图像特征进行提取和学习。另外，它计算简单，能够加快网络的训练速度。像在 AlexNet 等经典的神经网络模型中就成功应用了 ReLU 激活函数，提升了模型的性能和训练效率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064faca",
   "metadata": {},
   "source": [
    "我们创建一个神经网络`NeuralNetwork`的instance，将其移动到指定`device`上，然后打印出它的结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d875cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85dc082",
   "metadata": {},
   "source": [
    "为了使用该模型，我们向其传入the input data。这会执行模型的 `forward` 方法，同时还会进行一些后台操作。切勿直接调用 `model.forward ()` !!!\n",
    "\n",
    "调用该模型处理输入数据时，会返回一个二维张量。其中，维度 0 对应每个类别的 10 个原始预测值的每个输出，维度 1 对应每个输出的各个单独值。我们通过将其传递给 `nn.Softmax` 模块的一个实例来获得预测概率。\n",
    "- 举个例子：假设我们有一个三分类的模型（比如识别猫、狗、鸟），输入一张图片后，模型输出的原始预测值是一个包含 3 个元素的数组，例如 [1.2, 0.8, -0.3]。这些数值是模型对每个类别的 “打分”，但它们的范围不确定，也不能直接解释为概率。通过 `Softmax` 函数处理后，这些原始分数会被转换为概率分布，例如 [0.54, 0.38, 0.08]，表示图片属于猫、狗、鸟的概率分别为 54%、38% 和 8%。我们可以直接选择概率最高的类别（这里是猫）作为最终预测结果\n",
    "- 对于输入向量 $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$，Softmax 函数输出的第 i 个元素为：$\\text{Softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5747b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 生成一个随机输入张量，模拟一张28×28的单通道图像\n",
    "# torch.rand(1, 28, 28) 创建一个形状为 [1, 28, 28] 的张量\n",
    "# device=device 指定张量存储在指定设备（CPU/GPU）上\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "\n",
    "# 将输入传递给模型，获得原始预测值（logits）\n",
    "# logits 的形状通常为 [batch_size, num_classes]\n",
    "logits = model(X)\n",
    "\n",
    "# 应用Softmax函数将logits转换为概率分布\n",
    "# dim=1 表示对第二个维度（类别维度）进行Softmax操作\n",
    "# pred_probab 的形状与logits相同，但每个样本的类别分数之和为1\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "\n",
    "# 获取概率最高的类别索引\n",
    "# argmax(1) 表示在第二个维度（类别维度）上取最大值的索引\n",
    "# y_pred 是一个形状为 [batch_size] 的张量，包含预测的类别标签\n",
    "y_pred = pred_probab.argmax(1)\n",
    "\n",
    "# 打印预测结果\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b8da9",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "让我们剖析 FashionMNIST 模型中的各个层。为了说明这一点，我们将选取一个包含 3 张 28x28 尺寸图像的小批量样本，看看当它在网络中传递时会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281b7e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个形状为[3, 28, 28]的随机张量，\n",
    "# 模拟一个3通道（如RGB）的28×28像素图像\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "\n",
    "# 打印张量的维度信息，输出应为torch.Size([3, 28, 28])\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edd961",
   "metadata": {},
   "source": [
    "## nn.Flatten\n",
    "We initialize the [`nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer to convert each 2D 28x28 image into a contiguous（不间断的） array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab0910d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767614e9",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "\n",
    "The [`linear layer`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f681e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e155e185",
   "metadata": {},
   "source": [
    "## nn.ReLU\n",
    "非线性激活函数构建了模型输入与输出之间的复杂映射关系。它们在线性变换之后应用，用于引入非线性，帮助神经网络学习各种各样的现象。\n",
    "\n",
    "在这个模型中，我们在线性层之间使用了神经网络中的 [`ReLU` 函数](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)，但还有其他激活函数可用于在模型中引入非线性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c505fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.2594, -0.3404,  0.0543,  0.0757,  0.5511,  0.3468,  0.0217,  0.0199,\n",
      "          0.0876, -0.3879,  0.5041,  0.0974, -0.4897,  0.3758, -0.2685, -0.0617,\n",
      "         -0.0566, -0.9164,  0.2012, -0.3498],\n",
      "        [ 0.2341, -0.3851, -0.1324, -0.1336,  0.5921,  0.1998,  0.2289, -0.3141,\n",
      "          0.1073, -0.5461,  0.5249,  0.3040, -0.1568,  0.3868, -0.0042, -0.2018,\n",
      "         -0.0434, -0.5307,  0.0763, -0.5658],\n",
      "        [ 0.2474, -0.5597,  0.1563,  0.1173,  0.6961,  0.0620,  0.1685, -0.2289,\n",
      "          0.0476, -0.3720,  0.7633,  0.1478, -0.4536,  0.3780,  0.1305,  0.0417,\n",
      "         -0.1380, -0.6709,  0.3664, -0.4156]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.2594, 0.0000, 0.0543, 0.0757, 0.5511, 0.3468, 0.0217, 0.0199, 0.0876,\n",
      "         0.0000, 0.5041, 0.0974, 0.0000, 0.3758, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2012, 0.0000],\n",
      "        [0.2341, 0.0000, 0.0000, 0.0000, 0.5921, 0.1998, 0.2289, 0.0000, 0.1073,\n",
      "         0.0000, 0.5249, 0.3040, 0.0000, 0.3868, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0763, 0.0000],\n",
      "        [0.2474, 0.0000, 0.1563, 0.1173, 0.6961, 0.0620, 0.1685, 0.0000, 0.0476,\n",
      "         0.0000, 0.7633, 0.1478, 0.0000, 0.3780, 0.1305, 0.0417, 0.0000, 0.0000,\n",
      "         0.3664, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64657f5a",
   "metadata": {},
   "source": [
    "## nn.Sequential\n",
    "`nn.Sequential` 是一个有序的模块容器。数据会按照定义的相同顺序依次通过所有模块。你可以使用顺序容器，像如下  `seq_modules` 那样快速搭建一个网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e23218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2262, -0.1857,  0.2115,  0.0048, -0.2180, -0.0279,  0.2536,  0.0969,\n",
      "         -0.2188, -0.4782],\n",
      "        [-0.1118, -0.0789,  0.0946,  0.0347, -0.2986,  0.2486,  0.2058,  0.2379,\n",
      "         -0.2071, -0.4147],\n",
      "        [-0.0880, -0.1769,  0.1025,  0.0639, -0.1503,  0.1200,  0.2279,  0.1921,\n",
      "         -0.2319, -0.4943]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db58b59",
   "metadata": {},
   "source": [
    "## nn.Softmax\n",
    "神经网络的最后一个线性层返回 *logits*（即取值范围为 [-∞, ∞] 的原始值），这些值会被传递到 `nn.Softmax` 模块。*logits* 会被缩放至 [0, 1] 的取值范围，该范围的值代表模型对每个类别的预测概率。`dim` 参数表示沿着该维度，所有值的总和必须为 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ff4c28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0843, 0.0878, 0.1306, 0.1062, 0.0850, 0.1028, 0.1362, 0.1165, 0.0849,\n",
      "         0.0655],\n",
      "        [0.0899, 0.0929, 0.1105, 0.1041, 0.0746, 0.1289, 0.1235, 0.1275, 0.0817,\n",
      "         0.0664],\n",
      "        [0.0936, 0.0856, 0.1132, 0.1089, 0.0879, 0.1152, 0.1283, 0.1238, 0.0810,\n",
      "         0.0623]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fccab0",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "在神经网络中，许多层都有参数化设置 (*parameterized*)，也就是说，它们具有相关的*weights* 和 *biases*，这些 *weights* 和 *biases* 在训练过程中会进行优化。对 `nn.Module` 进行子类化会自动跟踪在模型对象内部定义的所有字段，并且可以通过模型的 `parameters()` 或 `named_parameters()` 方法访问所有参数。\n",
    "\n",
    "在这个例子中，我们遍历每个parameter，并打印其size以及其values的预览。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873ba65",
   "metadata": {},
   "source": [
    "| **方法**               | **返回值内容**                          | **使用场景**                          | **简单比喻**                          |\n",
    "|------------------------|---------------------------------------|-----------------------------------|-----------------------------------|\n",
    "| `model.parameters()`   | 只有参数值，没有名字                    | 当你想一次性处理所有参数时（如“全部参数乘以0.1”）     | 像一个装满零件的箱子，你只关心零件本身，不关心名字  |\n",
    "| `model.named_parameters()` | 参数值 + 参数名称（如`\"layer1.weight\"`） | 当你需要区分不同参数时（如“只训练最后一层”）       | 像一个贴了标签的零件箱，每个零件都标着“这是齿轮”“这是螺丝” |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d135e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0223,  0.0267,  0.0308,  ...,  0.0263, -0.0071, -0.0350],\n",
      "        [-0.0057,  0.0092,  0.0036,  ..., -0.0292,  0.0229,  0.0077]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0225,  0.0322], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0118, -0.0095,  0.0131,  ...,  0.0257, -0.0400,  0.0333],\n",
      "        [ 0.0264, -0.0146, -0.0189,  ...,  0.0398,  0.0343, -0.0200]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0315, -0.0210], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0114, -0.0048, -0.0117,  ...,  0.0391, -0.0338, -0.0263],\n",
      "        [-0.0183, -0.0358, -0.0355,  ...,  0.0062, -0.0165, -0.0409]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0155, 0.0374], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
